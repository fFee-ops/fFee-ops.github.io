---
title: TIDB简介及基础架构
date: 2022-10-30 16:32:39
tags: tidb 数据库 mysql
categories: TIDB
---

<!--more-->

# 1\. 什么是TIDB

TiDB 是一个分布式 NewSQL 数据库。它支持水平弹性扩展、ACID 事务、标准 SQL、MySQL 语法和 MySQL 协议，具有数据强一致的高可用特性，是一个不仅适合 OLTP 场景还适合 OLAP 场景的混合数据库。

## 1.1 什么是NewSQL

1.  SQL，传统关系型数据库，例如 MySQL
2.  noSQL，例如 MongoDB,Redis
3.  newSQL

### 1.1.1 传统SQL的问题

互联网在本世纪初开始迅速发展，互联网应用的用户规模、数据量都越来越大，并且要求7X24小时在线。传统关系型数据库在这种环境下成为了瓶颈，通常有2种解决方法：

①升级服务器硬件：虽然提升了性能，但总有天花板。  
②数据分片：数据涉及到跨库join、跨库事务时就很麻烦，很多人干脆自  
己在业务层处理，复杂度较高。

### 1.1.2 NoSQL 的问题

后来 noSQL 出现了，放弃了传统SQL的强事务保证和关系模型，重点放在数据库的高可用性和可扩展性。  
**优点：**

- 高可用性和可扩展性，自动分区，轻松扩展
- 不保证强一致性，性能大幅提升
- 没有关系模型的限制，极其灵活

**缺点**

- 不保证强一致性，对于普通应用没问题，但还是有不少像金融一样的企业级应用有强一致性的需求。
- 不支持 SQL 语句，兼容性是个大问题，不同的 NoSQL 数据库都有自己的 api 操作数据，比较复杂。

### 1.1.3 NewSQL 特性

NewSQL 提供了与 noSQL 相同的可扩展性，而且仍基于关系模型，还保留了极其成熟的 SQL 作为查询语言，保证了ACID事务特性。  
简单来讲，NewSQL 就是在传统关系型数据库上集成了 NoSQL 强大的可扩展性。

**NewSQL 的主要特性**

- SQL 支持，支持复杂查询和大数据分析。
- 支持 ACID 事务，支持隔离级别。
- 弹性伸缩，扩容缩容对于业务层完全透明。
- 高可用，自动容灾。

### 1.1.4 三种SQL的对比

![在这里插入图片描述](https://img-blog.csdnimg.cn/9b8b737dc7844efd8ca37d85293d70be.png)

## 1.2 TIDB核心特性

### 1.2.1 水平弹性扩展

通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。  
得益于 TiDB 存储计算分离的架构的设计，可按需对计算、存储分别进行在线扩容或者缩容，扩容或者缩容过程中对应用运维人员透明。

### 1.2.2 分布式事务支持

TiDB 100\% 支持标准的 ACID 事务

### 1.2.3 金融级高可用

相比于传统主从 \(M-S\) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100\% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 \(auto-failover\)，无需人工介入

数据采用多副本存储，数据副本通过 Multi-Raft 协议同步事务日志，多数派写入成功事务才能提交，确保数据强一致性且少数副本发生故障时不影响数据的可用性。可按需配置副本地理位置、副本数量等策略满足不同容灾级别的要求。

### 1.2.4 实时 HTAP

TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式HTAP 解决方案，一份存储同时处理 OLTP \& OLAP 无需传统繁琐的 ETL 过程

提供行存储引擎 TiKV、列存储引擎 TiFlash 两款存储引擎，TiFlash 通过 Multi-Raft Learner 协议实时从 TiKV 复制数据，确保行存储引擎 TiKV 和列存储引擎 TiFlash 之间的数据强一致。TiKV、TiFlash 可按需部署在不同的机器，解决 HTAP 资源隔离的问题。

### 1.2.5 云原生的分布式数据库

TiDB 是为云而设计的数据库，同 Kubernetes 深度耦合，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。TiDB 的设计目标是 100\% 的 OLTP 场景和 80\% 的 OLAP 场景，更复杂的OLAP 分析可以通过 TiSpark 项目来完成。 TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中  
间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力

### 1.4.6 高度兼容 MySQL

兼容 MySQL 5.7 协议、MySQL 常用的功能、MySQL 生态，应用无需或者修改少量代码即可从MySQL 迁移到 TiDB。

提供丰富的数据迁移工具帮助应用便捷完成数据迁移，大多数情况下，无需修改代码即可从 MySQL轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。

## 1.3 OLTP\&OLAP

### 1.3.1 OLTP\(联机事务处理\)

OLTP\(Online Transactional Processing\) 即联机事务处理，OLTP 是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，记录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易

联机事务处理是事务性非常高的系统，一般都是高可用的在线系统，以小的事务以及小的查询为主，评估其系统的时候，一般看其每秒执行的ransaction以及Execute SQL的数量。在这样的系统中，单个数据库每秒处理的Transaction往往超过几百个，或者是几千个，Select 语句的执行量每秒几千甚至几万个。典型的OLTP系统有电子商务系统、银行、证券等，如美国eBay的业务数据库，就是很典型的OLTP数据库

### 1.3.2 OLAP\(联机分析处理\)

OLAP\(Online Analytical Processing\) 即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。

典型的应用就是复杂的动态报表系统。在这样的系统中，语句的执行量不是考核标准，因为一条语句的执行时间可能会非常长，读取的数据也非常多。所以，在这样的系统中，考核的标准往往是磁盘子系统的吞吐量（带宽），如能达到多少MB/s的流量。

### 1.3.3 特性对比

OLTP和OLAP的特性对比  
![在这里插入图片描述](https://img-blog.csdnimg.cn/b79c47ac90874b3bb35dbe5dca991edc.png)

### 1.3.4 设计角度区别

![在这里插入图片描述](https://img-blog.csdnimg.cn/8331bcb928a1436898037ee574e93977.png)

# 2\. TiDB 整体架构

## 2.1 TiDB的优势

与传统的单机数据库相比，TiDB 具有以下优势:

- 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容
- 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直 接替换 MySQL
- 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明
- 支持 ACID 事务，对于一些有强一致需求的场景友好，例如:银行转账
- 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景

## 2.2 TiDB的组件

要深入了解 TiDB 的水平扩展和高可用特点，首先需要了解 TiDB 的整体架构。TiDB 集群主要包括 三个核心组件:TiDB Server，PD Server 和 TiKV Server，此外，还有用于解决用户复杂 OLAP 需求的TiSpark 组件。  
在内核设计上，TiDB 分布式数据库将整体架构拆分成了多个模块，各模块之间互相通信，组成完整 的 TiDB 系统。对应的架构图如下:  
![在这里插入图片描述](https://img-blog.csdnimg.cn/f718311b8d2541b7bb25fe32d6d8ca51.png)

### 2.2.1 TiDB Server

TiDB Server 负责接收 SQL 请求，处理 SQL 相关的逻辑，并通过 PD 找到存储计算所需数据的 TiKV 地址，与 TiKV 交互获取数据，最终返回结果。TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展，可以通过负载均衡组件\(如 LVS、HAProxy 或 F5\)对外提供统一的接入 地址。

### 2.2.2 PD \(Placement Driver\) Server

Placement Driver \(简称 PD\) 是整个集群的管理模块，其主要工作有三个:  
一是存储集群的元信息\(某个 Key 存储在哪个 TiKV 节点\);  
二是对 TiKV 集群进行调度和负载均衡\(如数据的迁移、Raft group leader 的迁移等\);  
三是分配全局唯一且递增的事务 ID。

PD 通过 Raft 协议保证数据的安全性。Raft 的 leader server 负责处理所有操作，其余的 PD server 仅用于保证高可用。建议部署**奇数个** PD 节点

### 2.2.3 TiKV Server

TiKV Server 负责**存储数据**，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储 数据的基本单位是 Region，每个 Region 负责存储一个 Key Range\(从 StartKey 到 EndKey 的左闭右 开区间\)的数据，每个 TiKV 节点会负责多个 Region。TiKV 使用 Raft 协议做复制，保持数据的一致性 和容灾。副本以 Region 为单位进行管理，不同节点上的多个 Region 构成一个 Raft Group，互为副 本。数据在多个 TiKV 之间的负载均衡由 PD 调度，这里也是以 Region 为单位进行调度。

### 2.2.4 TiSpark

TiSpark 作为 TiDB 中解决用户复杂 OLAP 需求的主要组件，将 Spark SQL 直接运行在 TiDB 存储层 上，同时融合 TiKV 分布式集群的优势，并融入大数据社区生态。至此，TiDB 可以通过一套系统，同时 支持 OLTP 与 OLAP，免除用户数据同步的烦恼。

### 2.2.5 TiFlash

TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的 形式进行存储，主要的功能是为分析型的场景加速。

## 2.3 TiKV整体架构

与传统的整节点备份方式不同的，TiKV是将数据按照 key 的范围划分成大致相等的切片（下文统称为 Region），每一个切片会有多个副本（通常是 3 个），其中一个副本是 Leader，提供读写服务。  
TiKV 通过 PD 对这些 Region 以及副本进行调度，以保证数据和读写负载都均匀地分散在各个 TiKV 上，这样的设计保证了整个集群资源的充分利用并且可以随着机器数量的增加水平扩展。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ae86790cdb3b437b94fd91cc1668e0e8.png)

### 2.3.1 Region分裂与合并

当某个 Region 的大小超过一定限制（默认是 144MB）后，TiKV 会将它分裂为两个或者更多个Region，以保证各个 Region 的大小是大致接近的，这样更有利于 PD 进行调度决策。同样，当某个Region 因为大量的删除请求导致 Region 的大小变得更小时，TiKV 会将比较小的两个相邻 Region 合并为一个。

### 2.3.2 Region调度

Region 与副本之间通过 Raft 协议来维持数据一致性，任何写请求都只能在 Leader 上写入，并且需要写入多数副本后（默认配置为 3 副本，即所有请求必须至少写入两个副本成功）才会返回客户端写入成功。

当 PD 需要把某个 Region 的一个副本从一个 TiKV 节点调度到另一个上面时，PD 会先为这个 RaftGroup 在目标节点上增加一个 Learner 副本（复制 Leader 的数据）。当这个 Learner 副本的进度大致追上 Leader 副本时，Leader 会将它变更为 Follower，之后再移除操作节点的 Follower 副本，这样就完成了 Region 副本的一次调度。

Leader 副本的调度原理也类似，不过需要在目标节点的 Learner 副本变为 Follower 副本后，再执行一次 Leader Transfer，让该 Follower 主动发起一次选举成为新 Leader，之后新 Leader 负责删除旧Leader 这个副本。

### 2.3.3 分布式事务

TiKV 支持分布式事务，用户（或者 TiDB）可以一次性写入多个 key-value 而不必关心这些 key-value是否处于同一个数据切片 \(Region\) 上，TiKV 通过两阶段提交保证了这些读写请求的 ACID 约束。

## 2.4 高可用架构

高可用是 TiDB 的另一大特点，TiDB/TiKV/PD 这三个组件都能容忍部分实例失效，不影响整个集群的可用性。下面分别说明这三个组件的可用性、单个实例失效后的后果以及如何恢复。

### 2.4.1 TiDB高可用

TiDB 是无状态的，推荐至少部署两个实例，前端通过负载均衡组件对外提供服务。当单个实例失效时，会影响正在这个实例上进行的 Session，从应用的角度看，会出现单次请求失败的情况，重新连接后即可继续获得服务。单个实例失效后，可以重启这个实例或者部署一个新的实例。

### 2.4.2 PD高可用

PD 是一个集群，通过 Raft 协议保持数据的一致性，单个实例失效时，如果这个实例不是 Raft 的leader，那么服务完全不受影响；如果这个实例是 Raft 的 leader，会重新选出新的 Raft leader，自动恢复服务。PD 在选举的过程中无法对外提供服务，这个时间大约是3秒钟。推荐至少部署三个 PD 实例，单个实例失效后，重启这个实例或者添加新的实例。

### 2.4.3 TiKV高可用

TiKV 是一个集群，通过 Raft 协议保持数据的一致性（副本数量可配置，默认保存三副本），并通过 PD 做负载均衡调度。单个节点失效时，会影响这个节点上存储的所有 Region。对于 Region 中的Leader 结点，会中断服务，等待重新选举；对于 Region 中的 Follower 节点，不会影响服务。当某个  
TiKV 节点失效，并且在一段时间内（默认 10 分钟）无法恢复，PD 会将其上的数据迁移到其他的 TiKV节点上。

## 2.5 应用场景

### 2.5.1 MySQL分片与合并

![在这里插入图片描述](https://img-blog.csdnimg.cn/6c2d3b9c897b42e5829ef5b445c9a744.png)  
![在这里插入图片描述](https://img-blog.csdnimg.cn/2490dee349394bf08d722e43dd21a341.png)  
TiDB 应用的第一类场景是 MySQL 的分片与合并。对于已经在用 MySQL 的业务，分库、分表、分片、中间件是常用手段，随着分片的增多，跨分片查询是一大难题。  
TiDB 在业务层兼容 MySQL 的访问协议，PingCAP 做了一个数据同步的工具——Syncer，它可以把TiDB 作为一个 MySQL Slave，将 TiDB 作为现有数据库的从库接在主 MySQL 库的后方，在这一层将数据打通，可以直接进行复杂的跨库、跨表、跨业务的实时 SQL 查询。

### 2.5.2 直接替换MySQL

在一个 TiDB 的数据库上，所有业务场景不需要做分库分表，所有的分布式工作都由数据库层完成。TiDB 兼容 MySQL 协议，所以可以直接替换 MySQL，而且基本做到了开箱即用，完全不用担心传统分库分表方案带来繁重的工作负担和复杂的维护成本，友好的用户界面让常规的技术人员可以高效地

进行维护和管理。另外，TiDB 具有 NoSQL 类似的扩容能力，在数据量和访问流量持续增长的情况下能够通过水平扩容提高系统的业务支撑能力，并且响应延迟稳定。

### 2.5.3 数据仓库

TiDB 本身是一个分布式系统，第三种使用场景是将 TiDB 当作数据仓库使用。TPC-H 是数据分析领域的一个测试集，TiDB 2.0 在 OLAP 场景下的性能有了大幅提升，原来只能在数据仓库里面跑的一些复杂的 Query，在 TiDB 2.0 里面跑，时间基本都能控制在 10 秒以内。当然，因为 OLAP 的范畴非常大，  
TiDB 的 SQL 也有搞不定的情况，为此 PingCAP 开源了 TiSpark，TiSpark 是一个 Spark 插件，用户可以直接用 Spark SQL 实时地在 TiKV 上做大数据分析。